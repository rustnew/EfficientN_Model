use serde::{Deserialize, Serialize};

/// Full CNN model configuration for malaria detection
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelConfig {
    /// Input image width
    pub image_width: usize,
    /// Input image height
    pub image_height: usize,
    /// Number of channels (3 for RGB, 1 for grayscale)
    pub image_channels: usize,
    /// EfficientNet variant (b0, b1, b2, etc.)
    pub efficientnet_variant: String,
    /// Dropout rate for regularization
    pub dropout_rate: f64,
    /// Units for the first fully-connected layer
    pub fc1_units: usize,
    /// Units for the second fully-connected layer
    pub fc2_units: usize,
    /// Number of species classes (5: Falciparum/Malariae/Ovale/Vivax/Uninfected)
    pub num_species_classes: usize,
    /// Number of stage classes (4: R/T/S/G)
    pub num_stage_classes: usize,
    /// Weight for stage loss in multi-task objective
    pub stage_loss_lambda: f64,
    /// Learning rate for the optimizer
    pub learning_rate: f64,
    /// Training batch size (4 recommand√© pour WGPU)
    pub batch_size: usize,
    /// Number of training epochs
    pub num_epochs: usize,
    /// Path to the data directory
    pub data_path: String,
    /// Ratio for train/validation split
    pub train_val_split: f64,
    /// Whether to use data caching
    pub use_cache: bool,
    /// Number of workers for data loading (0 pour WGPU)
    pub num_workers: usize,
    /// Gradient accumulation steps
    pub grad_accum_steps: usize,
}

impl Default for ModelConfig {
    fn default() -> Self {
        Self {
            // ‚úÖ Taille r√©duite pour WGPU
            image_width: 128,
            image_height: 128,
            image_channels: 3,
            efficientnet_variant: "b0".to_string(),
            dropout_rate: 0.3,
            fc1_units: 1280,
            fc2_units: 512,
            num_species_classes: 5,
            num_stage_classes: 4,
            stage_loss_lambda: 0.25,
            learning_rate: 0.001,
            // ‚úÖ Batch size r√©duit pour WGPU
            batch_size: 4,
            num_epochs: 10,
            data_path: "data".to_string(),
            train_val_split: 0.8,
            use_cache: true,
            // ‚úÖ ZERO workers pour WGPU
            num_workers: 0,
            grad_accum_steps: 1,
        }
    }
}



use anyhow::{anyhow, Result};
use burn::{
    data::{dataloader::batcher::Batcher, dataset::Dataset},
    tensor::{backend::Backend, Int, Tensor},
};
use image::{imageops::FilterType, ImageReader};
use rand::{rngs::StdRng, seq::SliceRandom, SeedableRng};
use rayon::prelude::*;
use std::{
    fs,
    path::{Path, PathBuf},
};

// Normalisation ImageNet
const IMAGENET_MEAN: [f32; 3] = [0.485, 0.456, 0.406];
const IMAGENET_STD: [f32; 3] = [0.229, 0.224, 0.225];

#[derive(Debug, Clone, PartialEq)]
pub struct ImageLabels {
    pub species: u8,      // 0: Falciparum, 1: Malariae, 2: Ovale, 3: Vivax, 4: Uninfected
    pub stages: [u8; 4],  // R, T, S, G (multi-label)
    pub infected: u8,     // 0: Non infect√©, 1: Infect√©
}

#[derive(Debug, Clone)]
pub struct MalariaImageItem {
    pub image_path: PathBuf,
    pub labels: ImageLabels,
}

#[derive(Debug, Clone)]
pub struct MalariaDataset {
    pub items: Vec<MalariaImageItem>,
    pub target_height: usize,
    pub target_width: usize,
}

impl MalariaDataset {
    pub fn new<P: AsRef<Path>>(
        data_dir: P,
        target_height: usize,
        target_width: usize,
        _use_cache: bool,
    ) -> Result<Self> {
        let data_dir = data_dir.as_ref();
        println!("üìÇ Chargement des donn√©es depuis: {}", data_dir.display());

        let image_files = Self::collect_image_files(data_dir)?;
        println!("üìä Nombre total d'images trouv√©es: {}", image_files.len());

        let items: Vec<MalariaImageItem> = image_files
            .par_iter()
            .filter_map(|path| {
                Self::parse_labels_from_path(path)
                    .map_err(|e| {
                        eprintln!("‚ö†Ô∏è  Impossible de parser {}: {}", path.display(), e);
                    })
                    .ok()
            })
            .collect();

        if items.is_empty() {
            return Err(anyhow!("Aucune image avec des labels valides trouv√©e!"));
        }

        let mut items = items;
        let mut rng = StdRng::seed_from_u64(42);
        items.shuffle(&mut rng);

        Self::print_statistics(&items);

        Ok(Self {
            items,
            target_height,
            target_width,
        })
    }

    fn collect_image_files(dir: &Path) -> Result<Vec<PathBuf>> {
        let mut files = Vec::new();
        let mut dirs = vec![dir.to_path_buf()];
        let valid_extensions = ["png", "jpg", "jpeg", "tif", "tiff", "bmp"];

        while let Some(current_dir) = dirs.pop() {
            let entries = fs::read_dir(&current_dir)
                .map_err(|e| anyhow!("Erreur lecture dossier {}: {}", current_dir.display(), e))?;

            for entry in entries {
                let entry = entry?;
                let path = entry.path();
                
                if path.is_dir() {
                    dirs.push(path);
                } else if path.is_file() {
                    if let Some(ext) = path.extension() {
                        let ext = ext.to_string_lossy().to_lowercase();
                        if valid_extensions.iter().any(|&e| e == ext) {
                            files.push(path);
                        }
                    }
                }
            }
        }
        Ok(files)
    }

    fn parse_labels_from_path(path: &Path) -> Result<MalariaImageItem> {
        let labels = Self::parse_labels_from_filename(path)?;
        
        Ok(MalariaImageItem {
            image_path: path.to_path_buf(),
            labels,
        })
    }

    fn parse_labels_from_filename(path: &Path) -> Result<ImageLabels> {
        let filename = path.file_stem()
            .ok_or_else(|| anyhow!("Nom de fichier invalide: {}", path.display()))?
            .to_string_lossy()
            .to_lowercase();
        
        let species = if filename.contains("uninfected") || filename.contains("non-infected") {
            4
        } else if filename.contains("falciparum") && !filename.contains("non-falciparum") {
            0
        } else if filename.contains("malariae") {
            1
        } else if filename.contains("ovale") {
            2
        } else if filename.contains("vivax") {
            3
        } else {
            0
        };
        
        let mut stages = [0u8; 4];
        let filename_upper = filename.to_uppercase();
        
        if filename_upper.contains("-R") || filename.contains("ring") {
            stages[0] = 1;
        }
        if filename_upper.contains("-T") || filename.contains("troph") {
            stages[1] = 1;
        }
        if filename_upper.contains("-S") || filename.contains("schizont") {
            stages[2] = 1;
        }
        if filename_upper.contains("-G") || filename.contains("gameto") {
            stages[3] = 1;
        }
        
        let infected = if species == 4 { 0 } else { 1 };
        if infected == 1 && stages.iter().all(|&s| s == 0) {
            stages = [1, 1, 1, 1];
        }
        
        Ok(ImageLabels {
            species,
            stages,
            infected,
        })
    }

    fn print_statistics(items: &[MalariaImageItem]) {
        let total = items.len();
        let infected: usize = items.iter().filter(|i| i.labels.infected == 1).count();
        let uninfected = total - infected;
        
        let mut species_counts = [0; 5];
        for item in items {
            species_counts[item.labels.species as usize] += 1;
        }
        
        let mut stage_counts = [0; 4];
        for item in items {
            for i in 0..4 {
                if item.labels.stages[i] == 1 {
                    stage_counts[i] += 1;
                }
            }
        }
        
        println!("üìä Statistiques du dataset:");
        println!("   ‚Ä¢ Total: {} images", total);
        println!("   ‚Ä¢ Infect√©: {} ({}%)", infected, infected * 100 / total.max(1));
        println!("   ‚Ä¢ Non-infect√©: {} ({}%)", uninfected, uninfected * 100 / total.max(1));
        println!("   ‚Ä¢ Esp√®ces:");
        let species_names = ["Falciparum", "Malariae", "Ovale", "Vivax", "Uninfected"];
        for (i, name) in species_names.iter().enumerate() {
            println!("     - {}: {}", name, species_counts[i]);
        }
        println!("   ‚Ä¢ Stades (multi-label):");
        let stage_names = ["Ring (R)", "Trophozoite (T)", "Schizont (S)", "Gametocyte (G)"];
        for (i, name) in stage_names.iter().enumerate() {
            println!("     - {}: {}", name, stage_counts[i]);
        }
    }

    pub fn load_and_preprocess_image(
        path: &Path,
        target_height: usize,
        target_width: usize,
    ) -> Result<Vec<f32>> {
        let img = ImageReader::open(path)?.decode()?.resize_exact(
            target_width as u32,
            target_height as u32,
            FilterType::Triangle,
        );

        let rgb_img = img.to_rgb8();
        let raw_pixels = rgb_img.into_raw();
        let frame_size = target_height * target_width;
        
        let mut chw_data = vec![0.0; frame_size * 3];
        
        for i in 0..frame_size {
            let base = i * 3;
            chw_data[i] = (raw_pixels[base] as f32 / 255.0 - IMAGENET_MEAN[0]) / IMAGENET_STD[0];
            chw_data[i + frame_size] = (raw_pixels[base + 1] as f32 / 255.0 - IMAGENET_MEAN[1]) / IMAGENET_STD[1];
            chw_data[i + 2 * frame_size] = (raw_pixels[base + 2] as f32 / 255.0 - IMAGENET_MEAN[2]) / IMAGENET_STD[2];
        }
        
        Ok(chw_data)
    }

    pub fn split(&self, ratio: f32) -> (Self, Self) {
        assert!(ratio > 0.0 && ratio < 1.0, "Ratio doit √™tre entre 0 et 1");
        let split_index = (self.items.len() as f32 * ratio) as usize;

        let train_items = self.items[..split_index].to_vec();
        let valid_items = self.items[split_index..].to_vec();

        println!("üìà Split du dataset (ratio: {}):", ratio);
        println!("   - Entra√Ænement: {} images", train_items.len());
        println!("   - Validation: {} images", valid_items.len());

        (
            Self {
                items: train_items,
                target_height: self.target_height,
                target_width: self.target_width,
            },
            Self {
                items: valid_items,
                target_height: self.target_height,
                target_width: self.target_width,
            },
        )
    }

    pub fn len(&self) -> usize {
        self.items.len()
    }

    pub fn is_empty(&self) -> bool {
        self.items.is_empty()
    }

    pub fn get(&self, index: usize) -> Option<MalariaImageItem> {
        self.items.get(index).cloned()
    }
}

impl Dataset<MalariaImageItem> for MalariaDataset {
    fn get(&self, index: usize) -> Option<MalariaImageItem> {
        self.get(index)
    }

    fn len(&self) -> usize {
        self.len()
    }
}

/// Batch pour le mod√®le malaria
#[derive(Debug, Clone)]
pub struct MalariaBatch<B: Backend> {
    pub images: Tensor<B, 4>,
    pub species: Tensor<B, 1, Int>,
    pub stages: Tensor<B, 2>,
    pub infected: Tensor<B, 1, Int>,
}

/// Batcher optimis√©
#[derive(Debug, Clone)]
pub struct MalariaBatcher<B: Backend> {
    pub image_height: usize,
    pub image_width: usize,
    _phantom: std::marker::PhantomData<B>,
}

impl<B: Backend> MalariaBatcher<B> {
    pub fn new(image_height: usize, image_width: usize) -> Self {
        Self {
            image_height,
            image_width,
            _phantom: std::marker::PhantomData,
        }
    }
}
// ... existing code ...

impl<B: Backend> Batcher<B, MalariaImageItem, MalariaBatch<B>> for MalariaBatcher<B> {
    fn batch(&self, items: Vec<MalariaImageItem>, device: &B::Device) -> MalariaBatch<B> {
        let batch_size = items.len();
        let frame_size = self.image_height * self.image_width;
        
        // Pr√©-allocation
        let mut images_data = Vec::with_capacity(batch_size * 3 * frame_size);
        let mut species_data = Vec::with_capacity(batch_size);
        let mut stages_data = Vec::with_capacity(batch_size * 4);
        let mut infected_data = Vec::with_capacity(batch_size);
        
        // Remplir les donn√©es
        for item in &items {
            match MalariaDataset::load_and_preprocess_image(
                &item.image_path,
                self.image_height,
                self.image_width,
            ) {
                Ok(data) => images_data.extend_from_slice(&data),
                Err(_) => images_data.extend(vec![0.0; 3 * frame_size]),
            }
            
            species_data.push(item.labels.species as i64);
            infected_data.push(item.labels.infected as i64);
            
            for stage in &item.labels.stages {
                stages_data.push(*stage as f32);
            }
        }
        
        // Cr√©ation des tenseurs
        // FIX: Use Tensor::<B, 1> for 1D tensors and specify dimensions
        MalariaBatch {
            images: Tensor::<B, 1>::from_floats(images_data.as_slice(), device)
                .reshape([batch_size, 3, self.image_height, self.image_width]),
            species: Tensor::<B, 1, Int>::from_ints(species_data.as_slice(), device),
            stages: Tensor::<B, 1>::from_floats(stages_data.as_slice(), device)
                .reshape([batch_size, 4]),
            infected: Tensor::<B, 1, Int>::from_ints(infected_data.as_slice(), device),
        }
    }
}


use burn::{
    nn::{
        conv::{Conv2d, Conv2dConfig},
        Linear, LinearConfig, PaddingConfig2d, Relu, Sigmoid,
    },
    prelude::*,
    tensor::{backend::Backend, module::adaptive_avg_pool2d, Tensor},
};

/// Squeeze-and-Excitation block optimis√©
#[derive(Module, Debug)]
pub struct SqueezeExcite<B: Backend> {
    fc1: Linear<B>,
    fc2: Linear<B>,
    sigmoid: Sigmoid,
}

impl<B: Backend> SqueezeExcite<B> {
    pub fn new(channels: usize, reduction: usize, device: &B::Device) -> Self {
        let reduced_channels = channels / reduction;
        Self {
            fc1: LinearConfig::new(channels, reduced_channels).init(device),
            fc2: LinearConfig::new(reduced_channels, channels).init(device),
            sigmoid: Sigmoid::new(),
        }
    }

    pub fn forward(&self, x: Tensor<B, 4>) -> Tensor<B, 4> {
        let batch_size = x.dims()[0];
        let channels = x.dims()[1];

        // Global average pooling
        let y = adaptive_avg_pool2d(x.clone(), [1, 1]).reshape([batch_size, channels]);

        // Excitation
        let y = self.fc1.forward(y);
        let y = self.fc2.forward(y);
        let y = self.sigmoid.forward(y);

        // Reshape pour broadcast et multiplication
        x * y.reshape([batch_size, channels, 1, 1])
    }
}

/// MBConv block optimis√©
#[derive(Module, Debug)]
pub struct MBConv<B: Backend> {
    expand_conv: Option<Conv2d<B>>,
    depthwise_conv: Conv2d<B>,
    squeeze_excite: Option<SqueezeExcite<B>>,
    project_conv: Conv2d<B>,
    use_residual: bool,
}

impl<B: Backend> MBConv<B> {
    #[allow(clippy::too_many_arguments)]
    pub fn new(
        in_channels: usize,
        out_channels: usize,
        expand_ratio: f32,
        kernel_size: [usize; 2],
        stride: [usize; 2],
        reduction: usize,
        device: &B::Device,
    ) -> Self {
        let expanded_channels = (in_channels as f32 * expand_ratio) as usize;

        // Expansion phase optionnelle
        let expand_conv = if (expand_ratio - 1.0).abs() > f32::EPSILON {
            Some(Conv2dConfig::new([in_channels, expanded_channels], [1, 1]).init(device))
        } else {
            None
        };

        // Depthwise convolution optimis√©e
        let depthwise_conv = Conv2dConfig::new([expanded_channels, expanded_channels], kernel_size)
            .with_stride(stride)
            .with_padding(PaddingConfig2d::Same)
            .with_groups(expanded_channels)
            .init(device);

        // SE block optionnel
        let squeeze_excite = if reduction > 0 {
            Some(SqueezeExcite::new(expanded_channels, reduction, device))
        } else {
            None
        };

        // Projection 1x1
        let project_conv = Conv2dConfig::new([expanded_channels, out_channels], [1, 1]).init(device);

        let use_residual = stride == [1, 1] && in_channels == out_channels;

        Self {
            expand_conv,
            depthwise_conv,
            squeeze_excite,
            project_conv,
            use_residual,
        }
    }

    pub fn forward(&self, x: Tensor<B, 4>) -> Tensor<B, 4> {
        // Copie du tenseur seulement si n√©cessaire pour residual
        let identity = if self.use_residual { Some(x.clone()) } else { None };

        // Expansion
        let mut y = match &self.expand_conv {
            Some(conv) => conv.forward(x.clone()),
            None => x,
        };

        // Depthwise
        y = self.depthwise_conv.forward(y);

        // SE
        if let Some(se) = &self.squeeze_excite {
            y = se.forward(y);
        }

        // Projection
        y = self.project_conv.forward(y);

        // Residual
        if let Some(id) = identity {
            y + id
        } else {
            y
        }
    }
}

/// EfficientNet-B0 backbone optimis√©
#[derive(Module, Debug)]
pub struct EfficientNetB0<B: Backend> {
    conv_stem: Conv2d<B>,
    blocks: Vec<MBConv<B>>,
    conv_head: Conv2d<B>,
    relu: Relu,
    num_features: usize,
}

impl<B: Backend> EfficientNetB0<B> {
    pub fn new(device: &B::Device) -> Self {
        let blocks_config = vec![
            (1, 16, 1, 1, 3),
            (6, 24, 2, 2, 3),
            (6, 40, 2, 2, 5),
            (6, 80, 3, 2, 3),
            (6, 112, 3, 1, 5),
            (6, 192, 4, 2, 5),
            (6, 320, 1, 1, 3),
        ];

        // Stem
        let conv_stem = Conv2dConfig::new([3, 32], [3, 3])
            .with_stride([2, 2])
            .with_padding(PaddingConfig2d::Same)
            .init(device);

        let mut blocks = Vec::new();
        let mut in_channels = 32;

        for (t, c, n, s, k) in blocks_config {
            let out_channels = Self::round_channels(c as f32, 1.0, 8);

            for i in 0..n {
                let stride = if i == 0 { [s, s] } else { [1, 1] };
                blocks.push(MBConv::new(in_channels, out_channels, t as f32, [k, k], stride, 4, device));
                in_channels = out_channels;
            }
        }

        // Head
        let conv_head = Conv2dConfig::new([in_channels, 1280], [1, 1]).init(device);

        Self {
            conv_stem,
            blocks,
            conv_head,
            relu: Relu::new(),
            num_features: 1280,
        }
    }

    fn round_channels(channels: f32, width_multiplier: f32, divisor: usize) -> usize {
        let channels = channels * width_multiplier;
        let new_channels = (channels + divisor as f32 / 2.0).max(divisor as f32);
        ((new_channels as usize) / divisor) * divisor
    }

    pub fn forward_features(&self, x: Tensor<B, 4>) -> Tensor<B, 4> {
        let mut x = self.conv_stem.forward(x);
        for block in &self.blocks {
            x = block.forward(x);
        }
        x = self.conv_head.forward(x);
        self.relu.forward(x)
    }

    pub fn forward(&self, x: Tensor<B, 4>) -> Tensor<B, 2> {
        // Features
        let features = self.forward_features(x);

        // Global average pooling
        let batch_size = features.dims()[0];
        let pooled = adaptive_avg_pool2d(features, [1, 1]);

        // Flatten
        pooled.reshape([batch_size, self.num_features])
    }

    pub fn num_features(&self) -> usize {
        self.num_features
    }
}

use burn::{
    nn::{Dropout, DropoutConfig, Linear, LinearConfig, Relu},
    prelude::*,
    tensor::backend::Backend,
};

/// Species classification head
#[derive(Module, Debug)]
pub struct SpeciesHead<B: Backend> {
    fc1: Linear<B>,
    fc2: Linear<B>,
    dropout: Dropout,
    relu: Relu,
}

impl<B: Backend> SpeciesHead<B> {
    pub fn new(
        in_features: usize,
        hidden_features: usize,
        num_classes: usize,
        dropout_rate: f64,
        device: &B::Device,
    ) -> Self {
        Self {
            fc1: LinearConfig::new(in_features, hidden_features).init(device),
            fc2: LinearConfig::new(hidden_features, num_classes).init(device),
            dropout: DropoutConfig::new(dropout_rate).init(),
            relu: Relu::new(),
        }
    }

    pub fn forward(&self, x: Tensor<B, 2>) -> Tensor<B, 2> {
        let x = self.fc1.forward(x);
        let x = self.relu.forward(x);
        let x = self.dropout.forward(x);
        self.fc2.forward(x)
    }
}

/// Stage classification head (multi-label)
#[derive(Module, Debug)]
pub struct StageHead<B: Backend> {
    fc1: Linear<B>,
    fc2: Linear<B>,
    dropout: Dropout,
    relu: Relu,
}

impl<B: Backend> StageHead<B> {
    pub fn new(
        in_features: usize,
        hidden_features: usize,
        num_stages: usize,
        dropout_rate: f64,
        device: &B::Device,
    ) -> Self {
        Self {
            fc1: LinearConfig::new(in_features, hidden_features).init(device),
            fc2: LinearConfig::new(hidden_features, num_stages).init(device),
            dropout: DropoutConfig::new(dropout_rate).init(),
            relu: Relu::new(),
        }
    }

    pub fn forward(&self, x: Tensor<B, 2>) -> Tensor<B, 2> {
        let x = self.fc1.forward(x);
        let x = self.relu.forward(x);
        let x = self.dropout.forward(x);
        self.fc2.forward(x)
    }
}

use crate::{
    config::ModelConfig,
    efficientnet::EfficientNetB0,
    heads::{SpeciesHead, StageHead},
};
use burn::{
    module::Module,
    nn::Relu,
    tensor::{
        backend::{AutodiffBackend, Backend},
        loss::cross_entropy_with_logits,
        Int, Tensor,
    },
    train::{TrainOutput, TrainStep, ValidStep},
};

use crate::data::MalariaBatch;

/// Training output for metrics tracking
#[derive(Debug, Clone)]
pub struct ClassificationOutput<B: Backend> {
    pub loss: Tensor<B, 1>,
    pub species_output: Tensor<B, 2>,
    pub species_targets: Tensor<B, 1, Int>,
}

impl<B: Backend> burn::train::metric::ItemLazy for ClassificationOutput<B> {
    type ItemSync = Self;
    fn sync(self) -> Self::ItemSync {
        self
    }
}

impl<B: Backend> burn::train::metric::Adaptor<burn::train::metric::LossInput<B>>
    for ClassificationOutput<B>
{
    fn adapt(&self) -> burn::train::metric::LossInput<B> {
        burn::train::metric::LossInput::new(self.loss.clone())
    }
}

impl<B: Backend> burn::train::metric::Adaptor<burn::train::metric::AccuracyInput<B>>
    for ClassificationOutput<B>
{
    fn adapt(&self) -> burn::train::metric::AccuracyInput<B> {
        let predictions = self.species_output.clone().argmax(1).float();
        burn::train::metric::AccuracyInput::new(predictions, self.species_targets.clone())
    }
}

/// Malaria detection model with EfficientNet backbone
#[derive(Module, Debug)]
pub struct MalariaEfficientNet<B: Backend> {
    efficientnet: EfficientNetB0<B>,
    species_head: SpeciesHead<B>,
    stage_head: StageHead<B>,
    relu: Relu,
    stage_loss_lambda: f32,
    training: bool,
}

impl<B: Backend> MalariaEfficientNet<B> {
    pub fn new(config: &ModelConfig, device: &B::Device) -> Self {
        let efficientnet = EfficientNetB0::new(device);
        let num_features = efficientnet.num_features();

        let species_head = SpeciesHead::new(
            num_features,
            config.fc1_units,
            config.num_species_classes,
            config.dropout_rate,
            device,
        );

        let stage_head = StageHead::new(
            num_features,
            config.fc2_units,
            config.num_stage_classes,
            config.dropout_rate,
            device,
        );

        Self {
            efficientnet,
            species_head,
            stage_head,
            relu: Relu::new(),
            stage_loss_lambda: config.stage_loss_lambda as f32,
            training: true,
        }
    }

    /// Set training mode
    pub fn set_training(&mut self, training: bool) {
        self.training = training;
    }

    /// Forward pass through the entire model
    pub fn forward(&self, x: Tensor<B, 4>) -> (Tensor<B, 2>, Tensor<B, 2>) {
        let features = self.efficientnet.forward(x);
        
        let species_logits = self.species_head.forward(features.clone());
        let stage_logits = self.stage_head.forward(features);
        
        (species_logits, stage_logits)
    }

    /// Compute species classification loss (Cross-Entropy)
    fn compute_species_loss(
        &self,
        output: Tensor<B, 2>,
        targets: Tensor<B, 1, Int>,
    ) -> Tensor<B, 1> {
        // ‚úÖ CORRECTION : Convertir les targets one-hot en Float
        let num_classes = output.dims()[1];
        let targets_one_hot = targets.one_hot(num_classes).float(); // <-- Ajouter .float() ici
        
        let loss = cross_entropy_with_logits(output, targets_one_hot);
        loss.mean().unsqueeze()
    }

    /// Compute stage classification loss (Binary Cross-Entropy)
    fn compute_stage_loss(&self, logits: Tensor<B, 2>, targets: Tensor<B, 2>) -> Tensor<B, 1> {
        // ‚úÖ CORRECTION : BCE with logits - version stable
        let zeros = logits.zeros_like();
        let max_val = logits.clone().max_pair(zeros.clone());
        
        let bce_term = max_val - logits.clone() * targets.clone();
        let log_term = (logits.abs().neg().exp() + 1.0).log();
        
        let loss = bce_term + log_term;
        
        // Moyenne sur le batch
        loss.mean().unsqueeze()
    }
}

/// Training step implementation
impl<B: AutodiffBackend> TrainStep<MalariaBatch<B>, ClassificationOutput<B>> for MalariaEfficientNet<B> {
    fn step(&self, batch: MalariaBatch<B>) -> TrainOutput<ClassificationOutput<B>> {
        // ‚úÖ DEBUG : V√©rifier les shapes
        let (species_output, stage_output) = self.forward(batch.images);
        
        let species_loss = self.compute_species_loss(species_output.clone(), batch.species.clone());
        let stage_loss = self.compute_stage_loss(stage_output, batch.stages.clone());
        
        let loss = species_loss + stage_loss * self.stage_loss_lambda;
        
        let grads = loss.backward();

        TrainOutput::new(
            self,
            grads,
            ClassificationOutput {
                loss: loss.detach(),
                species_output: species_output.detach(),
                species_targets: batch.species,
            },
        )
    }
}

/// Validation step implementation
impl<B: Backend> ValidStep<MalariaBatch<B>, ClassificationOutput<B>> for MalariaEfficientNet<B> {
    fn step(&self, batch: MalariaBatch<B>) -> ClassificationOutput<B> {
        let (species_output, stage_output) = self.forward(batch.images);
        
        let species_loss = self.compute_species_loss(species_output.clone(), batch.species.clone());
        let stage_loss = self.compute_stage_loss(stage_output, batch.stages.clone());
        
        let loss = species_loss + stage_loss * self.stage_loss_lambda;

        ClassificationOutput {
            loss: loss.detach(),
            species_output,
            species_targets: batch.species,
        }
    }
}

use crate::{
    config::ModelConfig,
    data::{MalariaBatcher, MalariaDataset},
    malaria_model::MalariaEfficientNet,
};
use anyhow::{anyhow, Result};
use burn::{
    data::dataloader::DataLoaderBuilder,
    optim::{decay::WeightDecayConfig, AdamConfig},
    record::{CompactRecorder, Recorder, RecorderError},
    tensor::backend::AutodiffBackend,
    train::{
        metric::{AccuracyMetric, LossMetric},
        LearnerBuilder,
    },
    prelude::Module,
};
use std::sync::Arc;

pub struct MalariaTrainer<B: AutodiffBackend> {
    config: ModelConfig,
    device: B::Device,
}

impl<B: AutodiffBackend> MalariaTrainer<B> {
    pub fn new(config: ModelConfig, device: B::Device) -> Self {
        Self { config, device }
    }

    pub fn run(&self) -> Result<()> {
        println!("üöÄ D√©marrage de l'entra√Ænement");
        println!("üìä Configuration:");
        println!("   - Backend: {}", std::any::type_name::<B>());
        println!("   - Device: {:?}", self.device);
        println!("   - Image size: {}x{}", self.config.image_width, self.config.image_height);
        println!("   - Batch size: {}", self.config.batch_size);
        println!("   - Learning rate: {}", self.config.learning_rate);
        
        self.validate_config()?;
        
        let model = self.create_model();
        let (train_loader, valid_loader) = self.create_dataloaders()?;
        let optim = self.create_optimizer();
        
        self.train_model(model, optim, train_loader, valid_loader)?;
        
        Ok(())
    }
    
    fn validate_config(&self) -> Result<()> {
        if self.config.batch_size == 0 {
            return Err(anyhow!("Batch size must be > 0"));
        }
        if self.config.learning_rate <= 0.0 {
            return Err(anyhow!("Learning rate must be > 0"));
        }
        if self.config.train_val_split <= 0.0 || self.config.train_val_split >= 1.0 {
            return Err(anyhow!("Train/val split must be between 0 and 1"));
        }
        if self.config.num_epochs == 0 {
            return Err(anyhow!("Number of epochs must be > 0"));
        }
        Ok(())
    }
    
    fn create_model(&self) -> MalariaEfficientNet<B> {
        println!("üõ†Ô∏è  Cr√©ation du mod√®le MalariaEfficientNet...");
        let mut model = MalariaEfficientNet::new(&self.config, &self.device);
        model.set_training(true);
        
        // Estimation des param√®tres
        let num_params = self.count_model_parameters();
        println!("   - Mod√®le cr√©√© avec {:.2}M param√®tres", num_params / 1_000_000.0);
        
        model
    }
    
    fn count_model_parameters(&self) -> f32 {
        5.2 * 1_000_000.0
    }
    
    fn create_dataloaders(&self) -> Result<(
        Arc<dyn burn::data::dataloader::DataLoader<B, crate::data::MalariaBatch<B>>>,
        Arc<dyn burn::data::dataloader::DataLoader<B::InnerBackend, crate::data::MalariaBatch<B::InnerBackend>>>
    )> {
        println!("üìÅ Chargement du dataset depuis '{}'...", self.config.data_path);
        
        let dataset = MalariaDataset::new(
            &self.config.data_path,
            self.config.image_height,
            self.config.image_width,
            self.config.use_cache,
        )?;
        
        if dataset.is_empty() {
            return Err(anyhow!("Dataset is empty"));
        }
        
        println!("   - Total d'images: {}", dataset.len());
        
        let (train_data, valid_data) = dataset.split(self.config.train_val_split as f32);
        
        println!("   - Split: {} train, {} validation", train_data.len(), valid_data.len());
        
        let batcher_train = MalariaBatcher::<B>::new(
            self.config.image_height, 
            self.config.image_width
        );
        
        let batcher_valid = MalariaBatcher::<B::InnerBackend>::new(
            self.config.image_height, 
            self.config.image_width
        );
        
        println!("   - Batch size: {}", self.config.batch_size);
        println!("   - Workers: {}", self.config.num_workers);
        
        let train_loader = DataLoaderBuilder::<B, _, _>::new(batcher_train)
            .batch_size(self.config.batch_size)
            .shuffle(42)
            .num_workers(self.config.num_workers) // ‚¨ÖÔ∏è 0 pour WGPU
            .build(train_data);
            
        let valid_loader = DataLoaderBuilder::<B::InnerBackend, _, _>::new(batcher_valid)
            .batch_size(self.config.batch_size)
            .num_workers(self.config.num_workers) // ‚¨ÖÔ∏è 0 pour WGPU
            .build(valid_data);
            
        println!("‚úÖ DataLoaders cr√©√©s avec succ√®s");
        
        Ok((train_loader, valid_loader))
    }
    
    fn create_optimizer(&self) -> AdamConfig {
        AdamConfig::new()
            .with_weight_decay(Some(WeightDecayConfig::new(1e-4)))
            .with_beta_1(0.9)
            .with_beta_2(0.999)
    }
    
    fn train_model(
        &self,
        model: MalariaEfficientNet<B>,
        optim: AdamConfig,
        train_loader: Arc<dyn burn::data::dataloader::DataLoader<B, crate::data::MalariaBatch<B>>>,
        valid_loader: Arc<dyn burn::data::dataloader::DataLoader<B::InnerBackend, crate::data::MalariaBatch<B::InnerBackend>>>,
    ) -> Result<()> {
        println!("üéØ D√©marrage de l'entra√Ænement pour {} √©poques...", self.config.num_epochs);
        println!("   - Checkpoint: ./checkpoints");
        println!("   - M√©triques: Loss, Accuracy");
        
        let start_time = std::time::Instant::now();
        
        // Construction du learner
        let learner = LearnerBuilder::new("./checkpoints")
            .metric_train_numeric(AccuracyMetric::new())
            .metric_valid_numeric(AccuracyMetric::new())
            .metric_train_numeric(LossMetric::new())
            .metric_valid_numeric(LossMetric::new())
            .with_file_checkpointer(CompactRecorder::new())
            .num_epochs(self.config.num_epochs)
            .grads_accumulation(self.config.grad_accum_steps)
            .summary()
            .build(model, optim.init(), self.config.learning_rate);
        
        println!("üöÄ Lancement de l'entra√Ænement...");
        let trained_model = learner.fit(train_loader, valid_loader);
        
        let duration = start_time.elapsed();
        println!("‚è±Ô∏è  Entra√Ænement termin√© en {:?}", duration);
        
        // Sauvegarde du mod√®le
        self.save_model(trained_model.model)
            .map_err(|e| anyhow!("Erreur de sauvegarde: {}", e))?;
        
        Ok(())
    }
    
    fn save_model<B2: burn::tensor::backend::Backend>(&self, model: MalariaEfficientNet<B2>) -> Result<(), RecorderError> {
        println!("üíæ Sauvegarde du mod√®le entra√Æn√©...");
        
        let recorder = CompactRecorder::new();
        let record = model.into_record();
        
        recorder.record(record, "./checkpoints/final-model".into())?;
        
        println!("‚úÖ Mod√®le sauvegard√© dans: ./checkpoints/final-model");
        
        Ok(())
    }
}

mod config;
mod data;
mod efficientnet;
mod heads;
mod malaria_model;
mod training;

use crate::config::ModelConfig;
use crate::training::MalariaTrainer;
use anyhow::Result;
use burn::backend::{
    wgpu::{Wgpu, WgpuDevice},
    Autodiff,
};

type Backend = Autodiff<Wgpu<f32, i32>>;

fn main() -> Result<()> {
    println!("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó");
    println!("‚ïë     EFFICIENTNET B0 - MALARIA DETECTION              ‚ïë");
    println!("‚ïë  Multi-Task: Species + Stage Classification          ‚ïë");
    println!("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù");

    let device = WgpuDevice::default();

    let config = ModelConfig {
        image_width: 128,
        image_height: 128,
        efficientnet_variant: "b0".to_string(),
        batch_size: 4,
        num_epochs: 20,
        use_cache: true,
        num_workers: 4,
        learning_rate: 0.001,
        dropout_rate: 0.3,
        data_path: "data".to_string(),
        train_val_split: 0.8,
        ..Default::default()
    };

    println!("\n Configuration EfficientNet:");
    println!("   ‚Ä¢ Image: {}x{}", config.image_width, config.image_height);
    println!("   ‚Ä¢ EfficientNet: B0");
    println!("   ‚Ä¢ Batch size: {}", config.batch_size);
    println!("   ‚Ä¢ Dropout: {}", config.dropout_rate);
    println!("   ‚Ä¢ √âpoques: {}", config.num_epochs);
    println!("   ‚Ä¢ Cache: activ√©");
    println!("   ‚Ä¢ Device: {:?}\n", device);

    let trainer = MalariaTrainer::<Backend>::new(config, device);
    
    match trainer.run() {
        Ok(_) => {
            println!(" Programme termin√© avec succ√®s!");
            Ok(())
        }
        Err(e) => {
            eprintln!(" Erreur pendant l'entra√Ænement: {}", e);
            Err(e)
        }
    }
}

[package]
name = "multi_model"
version = "0.1.0"
edition = "2021"

[dependencies]
burn = { version = "0.19.0", features = ["train", "autodiff", "wgpu"] }
burn-train = "0.19.1"
burn-tensor = "0.19.1"
burn-nn = "0.19.1"
burn-core = "0.19.1"
burn-autodiff = "0.19.1"
burn-wgpu = "0.19.1"

image = "0.25"
anyhow = "1.0"
rand = "0.8"
rayon = "1.8"
serde = { version = "1.0", features = ["derive"] }


